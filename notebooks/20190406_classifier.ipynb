{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_data(folder_path):\n",
    "    '''\n",
    "    Loads the JSON files puts them into a Python list (X) of strings\n",
    "    where each element in the list is a single question. It also sotres the category labels\n",
    "    into another list (y)\n",
    "    :param folder_path: the folder path that has json file for each category containing questions and answers\n",
    "    :return:\n",
    "    '''\n",
    "    print(\"loading the data ... \")\n",
    "    counter = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    categories = []\n",
    "    for f in os.listdir(folder_path):\n",
    "        if f.endswith(\"json\"):  # read all json files\n",
    "            file_path = os.path.join(folder_path, f)\n",
    "            contents = json.load(open(file_path, 'r', encoding='utf-8'))\n",
    "            for html_item in contents:\n",
    "                X.append(html_item['question'])\n",
    "                y.append(counter)\n",
    "            categories.append(f.replace(\".json\", \"\"))\n",
    "            counter = counter + 1\n",
    "    print(\"current categories: \"+str(categories))\n",
    "    return X, y, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pre_process(X, y):\n",
    "    '''\n",
    "    Tokenize and test and train split\n",
    "    :param X: full data\n",
    "    :param y: full labels\n",
    "    :param MAX_SEQUENCE_LENGTH: maximum tokens in each training instance\n",
    "    :param MAX_VOCAB_SIZE:  maximum number of words in vocab\n",
    "    :return:\n",
    "    '''\n",
    "    # Tokenization\n",
    "    filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789'\n",
    "    tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, filters=filters)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    # Create a sequence of words\n",
    "    sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "    word2idx = tokenizer.word_index\n",
    " \n",
    "    X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    " \n",
    "    y_train = to_categorical(y)\n",
    "    \n",
    "    # Train-test-split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.01)\n",
    "\n",
    "    \n",
    "    pickle.dump(word2idx, open(REL_PATH + \"/word2idx.pkl\", \"wb\"))\n",
    "    pickle.dump(tokenizer, open(REL_PATH + \"/tokenizer.pkl\", \"wb\"))\n",
    "    return X_train, X_test, y_train, y_test, word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_embeddings(path, word_index):\n",
    "    '''\n",
    "    load the embeddings and create the matrix\n",
    "    :param path: the path to embeddings\n",
    "    :param word_index: the dictionary of the words to indexs\n",
    "    :param MAX_VOCAB_SIZE: maximum number of words in vocab\n",
    "    :return: the indexs to embedding dicionary, embedding matrix to be use for training\n",
    "    '''\n",
    "    # Create a word - vector embedding dictionary (loading only the embeddings for words in our dictionary)\n",
    "    print('Loading word vectors...')\n",
    "    embeddings_index = {}\n",
    "    with open(path + \"/vectors.txt\", 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            vector = line.split(' ')\n",
    "            word = vector[0]\n",
    "            if word in word_index:\n",
    "                embeddings_index[word] = np.array(vector[1:], dtype=np.float32)\n",
    "                \n",
    "    # Create embedding matrix to laod into keras layer\n",
    "    print('Creating the emebdding matrix')\n",
    "    embeddings_matrix = np.zeros((MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i < MAX_VOCAB_SIZE:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embeddings_matrix[i] = embedding_vector\n",
    "    return embeddings_index, embeddings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     53,
     96
    ]
   },
   "outputs": [],
   "source": [
    "class ClassifierModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, embeddings_matrix, n_classes, max_len=500, hidden_dim=128):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        \n",
    "        # Constuct from existing embeddings matrix\n",
    "        if type(embeddings_matrix) is np.ndarray:\n",
    "            self.embedding = tf.keras.layers.Embedding(\n",
    "                                embeddings_matrix.shape[0],\n",
    "                                embeddings_matrix.shape[1],\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=False)\n",
    "            \n",
    "        # embeddings matrix is just a tuple with vocab size and embedding_dim\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(embeddings_matrix[0], embeddings_matrix[1])\n",
    "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.CuDNNLSTM(hidden_dim, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.Dense(n_classes)\n",
    "        self.max_pool = tf.keras.layers.GlobalMaxPool1D()\n",
    "        \n",
    "    def call(self, inputs, probs=False):\n",
    "        \n",
    "        X = self.embedding(inputs)\n",
    "        X = self.lstm(X)\n",
    "        X = self.max_pool(X)\n",
    "        logits = self.dense(X)\n",
    "        if probs:\n",
    "            return tf.nn.softmax(logits, axis=-1)\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred_logits):\n",
    "        \"\"\"Computes the loss between predicted and true labels.\"\"\"\n",
    "        \n",
    "        return tf.losses.softmax_cross_entropy(y_true, y_pred_logits)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Computes and returns the confusion matrix and accuracy score on given validation X and y\"\"\"\n",
    "        \n",
    "        # Make sure X is tensor\n",
    "        if type(X) is np.ndarray:\n",
    "            X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "\n",
    "        y_pred = self.call(X, probs=True).numpy()\n",
    "        \n",
    "        # Makse sure y is a numpy array (dumb sklearn uses len())\n",
    "        if type(y) is not np.ndarray:\n",
    "            y = y.numpy()\n",
    "        acc = accuracy_score(np.argmax(y, axis=1), np.argmax(y_pred, axis=1))\n",
    "        cm = confusion_matrix(np.argmax(y, axis=1), np.argmax(y_pred, axis=1))\n",
    "        return acc, cm\n",
    "    \n",
    "\n",
    "class AoLClassifier:\n",
    "    \n",
    "    def __init__(self, embeddings_matrix=(30000, 300), max_len=500, n_classes=10, hidden_dim=128):\n",
    "        \n",
    "        self._max_len = max_len\n",
    "        self._load(hidden_dim, n_classes, embeddings_matrix)\n",
    "    \n",
    "    def _load(self, hidden_dim, n_classes, embeddings_matrix):\n",
    "        \"\"\"Load tokenizers and model.\"\"\"\n",
    "        \n",
    "        # ----- Load tokenizers and create word indices ----- #\n",
    "        self.tokenizer = pickle.load(open(\"tokenizers/tokenizer.pkl\", \"rb\"))\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        \n",
    "        # ----- Load embeddings ----- #\n",
    "        #_, embeddings_matrix = get_embeddings(\"../embeddings/\", self.word_index)\n",
    "    \n",
    "        # ----- Load model and weights ----- #\n",
    "        self.model = ClassifierModel(embeddings_matrix, n_classes, hidden_dim=hidden_dim)\n",
    "        self.model.load_weights(os.path.join(\"models\", \"classifier_\" + \"bidirectional_20epochs\"))\n",
    "        \n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"A very basic function to preprocess the texts. Needs to be imporved.\"\"\"\n",
    "          \n",
    "        \n",
    "        # Tokenize, pad and convert to tensor\n",
    "        inputs = self.tokenizer.texts_to_sequences([text])\n",
    "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
    "                                                           maxlen=self._max_len, \n",
    "                                                           padding='post') \n",
    "        inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n",
    "        return inputs\n",
    "    \n",
    "    def classify_text(self, raw_text):\n",
    "        \"\"\"Generates the title given the supplied raw text.\"\"\"\n",
    "        \n",
    "        # Preprocess question\n",
    "        processed_text = self._preprocess(raw_text)\n",
    "        # Make predictions and return\n",
    "        probs = self.model(processed_text, probs=True).numpy()\n",
    "        return probs\n",
    "\n",
    "    \n",
    "def run_epoch(dataset, model, optimizer, p_bar):\n",
    "    \"\"\"Iteares onces over the entire dataset.\"\"\"\n",
    "\n",
    "    for X_batch, y_batch in dataset:\n",
    "        \n",
    "        p_bar.update(1)\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred, _ = model(X_batch, probs=False)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = model.compute_loss(y_batch, y_pred)\n",
    "\n",
    "            # Compute gradients\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "            # Apply gradients\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Some verbose\n",
    "        p_bar.set_postfix_str(\"Training loss: {0:.3f}\".format(loss.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REL_PATH = 'classifier'\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 300\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the data ... \n",
      "current categories: ['Arbeitsrecht', 'Erbrecht', 'Familienrecht', 'Kaufrecht', 'Mietrecht _ Wohnungseigentum', 'Oeffentlichesrecht', 'Sozialversicherungsrecht', 'Steuerrecht', 'Strafrecht', 'Vertragsrecht']\n"
     ]
    }
   ],
   "source": [
    "X, y, categories = load_data(\"../Dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, word2idx = pre_process(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99000, 500)\n",
      "(1000, 500)\n",
      "(99000, 10)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Creating the emebdding matrix\n"
     ]
    }
   ],
   "source": [
    "embeddings_index, embeddings_matrix = get_embeddings(\"../embeddings/\", word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the inputs to a Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(X_train.shape[0]).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierModel(embeddings_matrix, len(categories))\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "model.save_weights(os.path.join(\"models\", \"classifier_\" + \"bidirectional_20epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = AoLClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04171598, 0.11093291, 0.07323458, 0.08582268, 0.03053827,\n",
       "        0.16059865, 0.15863264, 0.10529935, 0.1893417 , 0.04388329]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.classify_text(\"Hallo, mein Name is Mohhammed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AttentionClassifierModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, embeddings_matrix, n_classes, max_len=500, hidden_dim=128):\n",
    "        super(AttentionClassifierModel, self).__init__()\n",
    "        \n",
    "        # Constuct from existing embeddings matrix\n",
    "        if type(embeddings_matrix) is np.ndarray:\n",
    "            self.embedding = tf.keras.layers.Embedding(\n",
    "                                embeddings_matrix.shape[0],\n",
    "                                embeddings_matrix.shape[1],\n",
    "                                weights=[embeddings_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=True)\n",
    "        \n",
    "        # Construct a new embedings matrix\n",
    "        # Embeddings matrix is just a tuple with vocab size and embedding_dim\n",
    "        else:\n",
    "            self.embedding = tf.keras.layers.Embedding(embeddings_matrix[0], embeddings_matrix[1])\n",
    "        self.lstm = tf.keras.layers.Bidirectional(\n",
    "                                        tf.keras.layers.CuDNNLSTM(hidden_dim, \n",
    "                                                        return_state=True,\n",
    "                                                        return_sequences=True))\n",
    "        self.attention = BahdanauAttention(hidden_dim)\n",
    "        self.dense = tf.keras.layers.Dense(64, activation='elu')\n",
    "        self.cls = tf.keras.layers.Dense(n_classes)\n",
    "\n",
    "        \n",
    "    def call(self, inputs, probs=False):\n",
    "        \n",
    "        X = self.embedding(inputs)\n",
    "        X = self.lstm(X)\n",
    "        output = X[0]\n",
    "        hiddens = tf.concat(X[1:], axis=-1)\n",
    "        context, attention_weights = self.attention(hiddens, output)\n",
    "        X = self.dense(context)\n",
    "        logits = self.cls(X)\n",
    "        if probs:\n",
    "            return tf.nn.softmax(logits, axis=-1), attention_weights\n",
    "        return logits, attention_weights\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred_logits):\n",
    "        \"\"\"Computes the loss between predicted and true labels.\"\"\"\n",
    "        \n",
    "        return tf.losses.softmax_cross_entropy(y_true, y_pred_logits)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"Computes and returns the confusion matrix and accuracy score on given validation X and y\"\"\"\n",
    "        \n",
    "        # Make sure X is tensor\n",
    "        if type(X) is np.ndarray:\n",
    "            X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "\n",
    "        y_pred, _ = self.call(X, probs=True)\n",
    "        y_pred = y_pred.numpy()\n",
    "        \n",
    "        # Makse sure y is a numpy array (dumb sklearn uses len())\n",
    "        if type(y) is not np.ndarray:\n",
    "            y = y.numpy()\n",
    "        acc = accuracy_score(np.argmax(y, axis=1), np.argmax(y_pred, axis=1))\n",
    "        cm = confusion_matrix(np.argmax(y, axis=1), np.argmax(y_pred, axis=1))\n",
    "        return acc, cm\n",
    "    \n",
    "\n",
    "\n",
    "        processed_text = self._preprocess(raw_text)\n",
    "        # Make predictions and return\n",
    "        probs = self.model(processed_text, probs=True).numpy()\n",
    "        return probs\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, hidden_size)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class AttentionAoLClassifier:\n",
    "    \n",
    "    def __init__(self, embeddings_matrix=(30000, 300), max_len=500, n_classes=10, hidden_dim=128):\n",
    "        \n",
    "        self._max_len = max_len\n",
    "        self._load(hidden_dim, n_classes, embeddings_matrix)\n",
    "    \n",
    "    def _load(self, hidden_dim, n_classes, embeddings_matrix):\n",
    "        \"\"\"Load tokenizers and model.\"\"\"\n",
    "        \n",
    "        # ----- Load tokenizers and create word indices ----- #\n",
    "        self.tokenizer = pickle.load(open(\"tokenizers/tokenizer.pkl\", \"rb\"))\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        \n",
    "        # ----- Load embeddings ----- #\n",
    "        #_, embeddings_matrix = get_embeddings(\"../embeddings/\", self.word_index)\n",
    "    \n",
    "        # ----- Load model and weights ----- #\n",
    "        self.model = AttentionClassifierModel(embeddings_matrix, n_classes, hidden_dim=hidden_dim)\n",
    "        self.model.load_weights(os.path.join(\"models\", \"classifier_attention_\" + \"bidirectional_15epochs\"))\n",
    "        \n",
    "    def _preprocess(self, text):\n",
    "        \"\"\"A very basic function to preprocess the texts. Needs to be imporved.\"\"\"\n",
    "          \n",
    "        \n",
    "        # Tokenize, pad and convert to tensor\n",
    "        inputs = self.tokenizer.texts_to_sequences([text])\n",
    "        inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, \n",
    "                                                           maxlen=self._max_len, \n",
    "                                                           padding='post') \n",
    "        inputs = tf.convert_to_tensor(inputs, dtype=tf.int32)\n",
    "        return inputs\n",
    "    \n",
    "    def classify_text(self, raw_text):\n",
    "        \"\"\"Generates the title given the supplied raw text.\"\"\"\n",
    "        \n",
    "        # Preprocess question\n",
    "        processed_text = self._preprocess(raw_text)\n",
    "        # Make predictions and return\n",
    "        probs, attn_weights = self.model(processed_text, probs=True)\n",
    "        probs, attn_weights = probs.numpy(), attn_weights.numpy()\n",
    "        return probs, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierModel(embeddings_matrix, len(categories))\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "for epoch in range(1, 6):\n",
    "    \n",
    "    # Show a nice progressbar during epoch\n",
    "    with tqdm(total=X_train.shape[0] // BATCH_SIZE + 1, desc='Epoch {}'.format(epoch)) as p_bar:\n",
    "        \n",
    "        # Run single epoch\n",
    "        run_epoch(dataset, model, optimizer, p_bar)\n",
    "        \n",
    "    # Validations score after epoch end\n",
    "    acc, cm = model.score(X_test, y_test)\n",
    "    print('Epoch: {}, accuracy: {}'.format(epoch, acc))\n",
    "    print('Confusion matrix: ', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save \n",
    "model.save_weights(os.path.join(\"models\", \"classifier_attention_\" + \"bidirectional_15epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.776, array([[86,  1,  1,  1,  2,  4,  6,  3,  2,  4],\n",
       "        [ 1, 90,  2,  0,  5,  0,  3,  2,  0,  2],\n",
       "        [ 0,  5, 71,  1,  2,  7,  5,  2,  2,  0],\n",
       "        [ 0,  1,  2, 80,  1,  0,  1,  3,  5,  6],\n",
       "        [ 0,  2,  0,  1, 88,  0,  2,  1,  0,  5],\n",
       "        [ 1,  0,  1,  2,  2, 74,  2,  2,  6,  3],\n",
       "        [ 5,  0,  6,  2,  2,  3, 81,  2,  1,  2],\n",
       "        [ 2,  5,  1,  3,  3,  8,  1, 71,  1,  0],\n",
       "        [ 2,  1,  2,  2,  4,  3,  2,  1, 75,  1],\n",
       "        [ 7,  0,  2, 19,  8,  5,  2,  0,  4, 60]], dtype=int64))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionClassifierModel(embeddings_matrix, len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x2a5e8501470>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(os.path.join(\"models\", \"classifier_attention_\" + \"bidirectional_15epochs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.776, array([[86,  1,  1,  1,  2,  4,  6,  3,  2,  4],\n",
       "        [ 1, 90,  2,  0,  5,  0,  3,  2,  0,  2],\n",
       "        [ 0,  5, 71,  1,  2,  7,  5,  2,  2,  0],\n",
       "        [ 0,  1,  2, 80,  1,  0,  1,  3,  5,  6],\n",
       "        [ 0,  2,  0,  1, 88,  0,  2,  1,  0,  5],\n",
       "        [ 1,  0,  1,  2,  2, 74,  2,  2,  6,  3],\n",
       "        [ 5,  0,  6,  2,  2,  3, 81,  2,  1,  2],\n",
       "        [ 2,  5,  1,  3,  3,  8,  1, 71,  1,  0],\n",
       "        [ 2,  1,  2,  2,  4,  3,  2,  1, 75,  1],\n",
       "        [ 7,  0,  2, 19,  8,  5,  2,  0,  4, 60]], dtype=int64))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = AttentionAoLClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"ich möchte meine wohnung verkaufen ein freund schilderte mir seine eigene bei der der käufer die kaufpreiszahlung um wegen angeblich eingetretenen mängeln zwischen und kaufpreiszahlung jetzt geht die sache vor gericht wie kann man sowas vermeiden etwa durch ein rücktrittsrecht für den verkäufer wegen des kaufpreises\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.882, array([[101,   0,   0,   0,   0,   1,   4,   0,   3,   1],\n",
       "        [  0,  77,   1,   0,   1,   1,   1,   1,   2,   1],\n",
       "        [  0,   3,  89,   0,   2,   2,   4,   1,   1,   0],\n",
       "        [  0,   3,   0,  98,   1,   1,   0,   2,   0,   9],\n",
       "        [  0,   0,   0,   1,  83,   1,   0,   0,   0,   4],\n",
       "        [  0,   2,   2,   0,   2,  83,   0,   0,   4,   3],\n",
       "        [  3,   2,   4,   0,   0,   0,  93,   1,   1,   0],\n",
       "        [  1,   2,   3,   0,   0,   3,   1, 103,   0,   1],\n",
       "        [  1,   0,   0,   0,   0,   1,   1,   1,  83,   0],\n",
       "        [  1,   2,   0,  15,   4,   2,   1,   1,   1,  72]], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify_text(txt)[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
